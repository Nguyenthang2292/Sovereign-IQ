# CNN-LSTM-Attention Model Pipeline

## ğŸ“‹ Tá»•ng quan

Pipeline nÃ y mÃ´ táº£ quy trÃ¬nh hoÃ n chá»‰nh Ä‘á»ƒ train vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh **CNN-LSTM vá»›i Attention Mechanism** cho viá»‡c dá»± Ä‘oÃ¡n tÃ­n hiá»‡u trading (LONG/SHORT/NONE) hoáº·c dá»± Ä‘oÃ¡n return.

**File:** `modules/lstm/signals_cnn_lstm_attention.py`

---

## ğŸ”„ Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    INPUT: OHLC DataFrame                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 1: Preprocessing                                          â”‚
â”‚  - Generate Technical Indicators                                â”‚
â”‚  - Create Targets (Classification/Regression)                  â”‚
â”‚  - Scale Features (MinMax/Standard)                            â”‚
â”‚  - Create Sliding Window Sequences                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 2: Data Splitting                                         â”‚
â”‚  - Train Set (70%)                                              â”‚
â”‚  - Validation Set (15%)                                         â”‚
â”‚  - Test Set (15%)                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 3: Model Creation                                          â”‚
â”‚  - CNN-LSTM-Attention Model                                     â”‚
â”‚  - LSTM-Attention Model                                         â”‚
â”‚  - Standard LSTM Model                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 4: Training                                               â”‚
â”‚  - GPU Setup & Mixed Precision                                  â”‚
â”‚  - Loss Function (FocalLoss/MSELoss)                           â”‚
â”‚  - Optimizer (AdamW)                                           â”‚
â”‚  - Learning Rate Scheduler                                      â”‚
â”‚  - Early Stopping                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  STEP 5: Threshold Optimization                                 â”‚
â”‚  - Grid Search for Optimal Threshold                            â”‚
â”‚  - Maximize Sharpe Ratio                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  OUTPUT: Trained Model + Metadata                               â”‚
â”‚  - Model State Dict                                             â”‚
â”‚  - Model Config                                                â”‚
â”‚  - Training History                                            â”‚
â”‚  - Optimal Threshold                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Chi tiáº¿t cÃ¡c bÆ°á»›c

### **STEP 1: Preprocessing** (`preprocess_cnn_lstm_data`)

#### Input:
- `df_input`: DataFrame chá»©a OHLC data
- `look_back`: Sá»‘ time steps Ä‘á»ƒ táº¡o sequence (default: `WINDOW_SIZE_LSTM`)
- `output_mode`: `'classification'` hoáº·c `'regression'`
- `scaler_type`: `'minmax'` hoáº·c `'standard'`

#### Quy trÃ¬nh:

1. **Generate Technical Indicators**
   ```python
   df = _generate_indicator_features(df_input.copy())
   ```
   - TÃ­nh toÃ¡n cÃ¡c technical indicators tá»« OHLC data
   - Sá»­ dá»¥ng cÃ¡c features tá»« `MODEL_FEATURES`

2. **Create Targets**
   - **Classification Mode:**
     ```python
     df = create_balanced_target(df, threshold=TARGET_THRESHOLD_LSTM, 
                                 neutral_zone=NEUTRAL_ZONE_LSTM)
     ```
     - Táº¡o 3 classes: LONG (-1), NONE (0), SHORT (1)
     - Dá»±a trÃªn future return vÃ  threshold
   
   - **Regression Mode:**
     ```python
     df['Target'] = df['close'].pct_change().shift(-1)
     ```
     - Target lÃ  future return

3. **Data Cleaning**
   - Drop NaN values
   - Validate data sufficiency
   - Handle invalid values (NaN, Inf)

4. **Feature Scaling**
   ```python
   scaler = MinMaxScaler()  # hoáº·c StandardScaler()
   scaled_features = scaler.fit_transform(features)
   ```

5. **Create Sliding Window Sequences**
   ```python
   for i in range(look_back, len(scaled_features)):
       sequence = scaled_features[i-look_back:i]
       X_sequences.append(sequence)
       y_targets.append(target_values[i])
   ```
   - Má»—i sequence cÃ³ shape: `(look_back, num_features)`
   - Táº¡o sequences liÃªn tiáº¿p tá»« data

#### Output:
- `X_sequences`: Array shape `(n_samples, look_back, num_features)`
- `y_targets`: Array shape `(n_samples,)`
- `scaler`: Fitted scaler object
- `feature_names`: List cÃ¡c features Ä‘Æ°á»£c sá»­ dá»¥ng

---

### **STEP 2: Data Splitting** (`split_train_test_data`)

#### Quy trÃ¬nh:

```python
X_train, X_val, X_test, y_train, y_val, y_test = split_train_test_data(
    X, y, 
    train_ratio=0.7,      # 70% train
    validation_ratio=0.15 # 15% validation
)
# Test set: 15% cÃ²n láº¡i
```

#### Validation:
- Kiá»ƒm tra X vÃ  y cÃ³ cÃ¹ng length
- Äáº£m báº£o Ä‘á»§ samples (tá»‘i thiá»ƒu 10)
- Validate ratios há»£p lá»‡

---

### **STEP 3: Model Creation** (`create_cnn_lstm_attention_model`)

#### CÃ¡c loáº¡i model:

1. **CNNLSTMAttentionModel** (khi `use_cnn=True`)
   - CNN layers Ä‘á»ƒ extract features
   - LSTM layers Ä‘á»ƒ capture temporal patterns
   - Attention mechanism Ä‘á»ƒ focus vÃ o important time steps
   - Output: Classification (3 classes) hoáº·c Regression (1 value)

2. **LSTMAttentionModel** (khi `use_attention=True` vÃ  `use_cnn=False`)
   - LSTM layers vá»›i Multi-Head Attention
   - KhÃ´ng cÃ³ CNN layers

3. **LSTMModel** (khi cáº£ `use_cnn=False` vÃ  `use_attention=False`)
   - Standard LSTM model

#### Parameters:
- `input_size`: Sá»‘ lÆ°á»£ng features
- `look_back`: Sequence length
- `output_mode`: `'classification'` hoáº·c `'regression'`
- `num_heads`: Sá»‘ attention heads (default tá»« `GPU_MODEL_CONFIG['nhead']`)
- `cnn_features`: 64
- `lstm_hidden`: 32
- `dropout`: 0.3

---

### **STEP 4: Training** (`train_cnn_lstm_attention_model`)

#### GPU Setup:

```python
gpu_available = check_gpu_availability()
device = torch.device('cuda:0' if gpu_available else 'cpu')
use_mixed_precision = gpu_available and torch.cuda.get_device_capability(0)[0] >= 7
```

- Tá»± Ä‘á»™ng detect GPU
- Sá»­ dá»¥ng mixed precision (FP16) náº¿u GPU há»— trá»£ (compute capability >= 7)
- Cáº¥u hÃ¬nh GPU memory

#### Training Configuration:

1. **Loss Function:**
   - Classification: `FocalLoss(alpha=0.25, gamma=2.0)`
   - Regression: `nn.MSELoss()`

2. **Optimizer:**
   ```python
   optimizer = optim.AdamW(
       model.parameters(), 
       lr=0.001, 
       weight_decay=0.01, 
       eps=1e-8
   )
   ```

3. **Learning Rate Scheduler:**
   ```python
   scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
       optimizer, 
       T_0=10,      # Initial period
       T_mult=2,    # Period multiplier
       eta_min=1e-6 # Minimum learning rate
   )
   ```

4. **Batch Size Optimization:**
   ```python
   optimal_batch_size = get_optimal_batch_size(device, input_size, look_back)
   if use_cnn:
       optimal_batch_size = max(4, optimal_batch_size // 8)
   ```
   - Tá»± Ä‘á»™ng tá»‘i Æ°u batch size dá»±a trÃªn GPU memory
   - CNN models sá»­ dá»¥ng batch size nhá» hÆ¡n

#### Training Loop:

**Training Phase:**
```python
for batch_X, batch_y in train_loader:
    # Forward pass
    outputs = model(batch_X)
    loss = criterion(outputs, batch_y)
    
    # Backward pass (vá»›i mixed precision náº¿u cÃ³)
    if use_mixed_precision:
        scaler_amp.scale(loss).backward()
        scaler_amp.step(optimizer)
    else:
        loss.backward()
        optimizer.step()
    
    # Gradient clipping
    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

**Validation Phase:**
```python
model.eval()
with torch.no_grad():
    for batch_X, batch_y in val_loader:
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        val_loss += loss.item()
```

**Early Stopping:**
- Patience: 10 epochs
- Monitor validation loss
- Restore best model state khi dá»«ng sá»›m

#### Metrics Logging:

- **Classification:**
  - Train/Validation Loss
  - Train/Validation Accuracy
  - Learning Rate

- **Regression:**
  - Train/Validation Loss
  - Learning Rate

---

### **STEP 5: Threshold Optimization**

#### Classification Mode:

```python
best_confidence, best_sharpe = threshold_optimizer.optimize_classification_threshold(
    test_predictions, test_returns
)
```

- Grid search cÃ¡c confidence thresholds
- TÃ­nh Sharpe ratio cho má»—i threshold
- Chá»n threshold tá»‘i Æ°u

#### Regression Mode:

```python
best_threshold, best_sharpe = threshold_optimizer.optimize_regression_threshold(
    test_predictions.flatten(), test_returns, prices
)
```

- Grid search cÃ¡c return thresholds
- TÃ­nh Sharpe ratio
- Chá»n threshold tá»‘i Æ°u

---

### **STEP 6: Model Saving**

#### Saved Information:

```python
save_dict = {
    'model_state_dict': model.state_dict(),
    'model_config': {
        'input_size': input_size,
        'look_back': look_back,
        'output_mode': output_mode,
        'use_cnn': use_cnn,
        'use_attention': use_attention,
        'attention_heads': attention_heads,
        'num_classes': num_classes
    },
    'training_info': {
        'epochs_trained': epoch + 1,
        'best_val_loss': best_val_loss,
        'final_lr': optimizer.param_groups[0]['lr']
    },
    'data_info': {
        'scaler': scaler,
        'feature_names': feature_names,
        'train_samples': len(X_train),
        'val_samples': len(X_val),
        'test_samples': len(X_test)
    },
    'optimization_results': {
        'optimal_threshold': threshold_optimizer.best_threshold,
        'best_sharpe': threshold_optimizer.best_sharpe
    },
    'training_history': {
        'train_loss': [...],
        'val_loss': [...]
    }
}
```

#### File Location:
- `MODELS_DIR / "cnn_lstm_attention_{output_mode}_model.pth"`

---

## ğŸ¯ CÃ¡c Functions chÃ­nh

### 1. `preprocess_cnn_lstm_data()`

**Má»¥c Ä‘Ã­ch:** Chuáº©n bá»‹ dá»¯ liá»‡u cho training

**Input:**
- `df_input`: DataFrame vá»›i OHLC data
- `look_back`: Sá»‘ time steps (default: `WINDOW_SIZE_LSTM`)
- `output_mode`: `'classification'` hoáº·c `'regression'`
- `scaler_type`: `'minmax'` hoáº·c `'standard'`

**Output:**
- `X_sequences`: Sequences array `(n_samples, look_back, num_features)`
- `y_targets`: Target array `(n_samples,)`
- `scaler`: Fitted scaler
- `feature_names`: List feature names

---

### 2. `split_train_test_data()`

**Má»¥c Ä‘Ã­ch:** Chia dá»¯ liá»‡u thÃ nh train/validation/test

**Input:**
- `X`: Input sequences
- `y`: Targets
- `train_ratio`: 0.7 (70%)
- `validation_ratio`: 0.15 (15%)

**Output:**
- `X_train, X_val, X_test, y_train, y_val, y_test`

---

### 3. `create_cnn_lstm_attention_model()`

**Má»¥c Ä‘Ã­ch:** Táº¡o model architecture

**Input:**
- `input_size`: Sá»‘ features
- `use_attention`: CÃ³ dÃ¹ng attention khÃ´ng
- `use_cnn`: CÃ³ dÃ¹ng CNN khÃ´ng
- `look_back`: Sequence length
- `output_mode`: `'classification'` hoáº·c `'regression'`

**Output:**
- Model object (CNNLSTMAttentionModel, LSTMAttentionModel, hoáº·c LSTMModel)

---

### 4. `train_cnn_lstm_attention_model()`

**Má»¥c Ä‘Ã­ch:** Train model hoÃ n chá»‰nh

**Input:**
- `df_input`: Input DataFrame
- `save_model`: CÃ³ lÆ°u model khÃ´ng (default: True)
- `epochs`: Sá»‘ epochs (default: `DEFAULT_EPOCHS`)
- `use_early_stopping`: CÃ³ dÃ¹ng early stopping khÃ´ng (default: True)
- `use_attention`: CÃ³ dÃ¹ng attention khÃ´ng (default: True)
- `use_cnn`: CÃ³ dÃ¹ng CNN khÃ´ng (default: True)
- `look_back`: Sequence length (default: `WINDOW_SIZE_LSTM`)
- `output_mode`: `'classification'` hoáº·c `'regression'` (default: `'classification'`)
- `attention_heads`: Sá»‘ attention heads (default: tá»« `GPU_MODEL_CONFIG`)

**Output:**
- `(trained_model, threshold_optimizer)`

---

### 5. `train_and_save_global_cnn_lstm_attention_model()`

**Má»¥c Ä‘Ã­ch:** Wrapper function Ä‘á»ƒ train vÃ  lÆ°u global model

**Input:**
- `combined_df`: Combined DataFrame tá»« nhiá»u trading pairs
- `model_filename`: Optional custom filename
- CÃ¡c parameters tÆ°Æ¡ng tá»± `train_cnn_lstm_attention_model()`

**Output:**
- `(trained_model, model_path_string)`

---

## âš™ï¸ Configuration Constants

CÃ¡c constants Ä‘Æ°á»£c import tá»« `livetrade.config`:

- `MODEL_FEATURES`: List cÃ¡c features sá»­ dá»¥ng trong model
- `WINDOW_SIZE_LSTM`: Default look_back window size
- `DEFAULT_EPOCHS`: Default sá»‘ epochs Ä‘á»ƒ train
- `TARGET_THRESHOLD_LSTM`: Threshold Ä‘á»ƒ táº¡o classification targets
- `NEUTRAL_ZONE_LSTM`: Neutral zone cho balanced targets
- `TRAIN_TEST_SPLIT`: Train ratio (default: 0.7)
- `VALIDATION_SPLIT`: Validation ratio (default: 0.15)
- `GPU_MODEL_CONFIG`: GPU configuration (bao gá»“m `nhead` cho attention)
- `MODELS_DIR`: Directory Ä‘á»ƒ lÆ°u models
- `COL_CLOSE`: Column name cho close price

---

## ğŸš€ VÃ­ dá»¥ sá»­ dá»¥ng

### Example 1: Train Classification Model

```python
import pandas as pd
from modules.lstm.signals_cnn_lstm_attention import train_cnn_lstm_attention_model

# Load price data
df = pd.read_csv('price_data.csv')

# Train model
model, threshold_optimizer = train_cnn_lstm_attention_model(
    df_input=df,
    epochs=100,
    use_cnn=True,
    use_attention=True,
    output_mode='classification',
    look_back=60
)

print(f"Optimal confidence threshold: {threshold_optimizer.best_threshold}")
print(f"Best Sharpe ratio: {threshold_optimizer.best_sharpe}")
```

### Example 2: Train Regression Model

```python
model, threshold_optimizer = train_cnn_lstm_attention_model(
    df_input=df,
    epochs=100,
    use_cnn=True,
    use_attention=True,
    output_mode='regression',
    look_back=60
)
```

### Example 3: Train Global Model

```python
from modules.lstm.signals_cnn_lstm_attention import train_and_save_global_cnn_lstm_attention_model

# Combined data from multiple symbols
combined_df = pd.concat([df_btc, df_eth, df_bnb], ignore_index=True)

model, model_path = train_and_save_global_cnn_lstm_attention_model(
    combined_df=combined_df,
    use_cnn=True,
    use_attention=True,
    output_mode='classification'
)

print(f"Model saved to: {model_path}")
```

---

## ğŸ”§ Advanced Features

### 1. Mixed Precision Training

- Tá»± Ä‘á»™ng enable khi GPU há»— trá»£ (compute capability >= 7)
- Sá»­ dá»¥ng FP16 Ä‘á»ƒ tÄƒng tá»‘c vÃ  giáº£m memory
- Gradient scaling Ä‘á»ƒ trÃ¡nh underflow

### 2. Early Stopping

- Patience: 10 epochs
- Monitor validation loss
- Tá»± Ä‘á»™ng restore best model state

### 3. Gradient Clipping

- Max norm: 1.0
- TrÃ¡nh gradient explosion

### 4. Batch Size Optimization

- Tá»± Ä‘á»™ng tá»‘i Æ°u dá»±a trÃªn GPU memory
- CNN models sá»­ dá»¥ng batch size nhá» hÆ¡n (Ã·8)

### 5. Error Handling & Fallback

- Kiá»ƒm tra data sufficiency
- Fallback to minimal features náº¿u thiáº¿u indicators
- Comprehensive error logging

---

## ğŸ“Š Model Architecture

### CNN-LSTM-Attention Model:

```
Input: (batch_size, look_back, num_features)
    â†“
CNN Layers (Feature Extraction)
    â†“
LSTM Layers (Temporal Patterns)
    â†“
Multi-Head Attention (Important Time Steps)
    â†“
Dense Layers
    â†“
Output: 
  - Classification: (batch_size, 3) - [LONG, NONE, SHORT]
  - Regression: (batch_size, 1) - [Return]
```

---

## ğŸ“ˆ Performance Metrics

### Classification Mode:
- **Accuracy**: Tá»· lá»‡ dá»± Ä‘oÃ¡n Ä‘Ãºng
- **Loss**: Focal Loss
- **Sharpe Ratio**: Tá»« threshold optimization

### Regression Mode:
- **MSE Loss**: Mean Squared Error
- **Sharpe Ratio**: Tá»« threshold optimization

---

## ğŸ’¾ Model Loading

```python
import torch
from modules.lstm.signals_cnn_lstm_attention import create_cnn_lstm_attention_model

# Load saved model
checkpoint = torch.load('cnn_lstm_attention_classification_model.pth')

# Recreate model
model = create_cnn_lstm_attention_model(
    input_size=checkpoint['model_config']['input_size'],
    look_back=checkpoint['model_config']['look_back'],
    output_mode=checkpoint['model_config']['output_mode'],
    use_cnn=checkpoint['model_config']['use_cnn'],
    use_attention=checkpoint['model_config']['use_attention'],
    attention_heads=checkpoint['model_config']['attention_heads']
)

# Load weights
model.load_state_dict(checkpoint['model_state_dict'])

# Get scaler and feature names
scaler = checkpoint['data_info']['scaler']
feature_names = checkpoint['data_info']['feature_names']
optimal_threshold = checkpoint['optimization_results']['optimal_threshold']
```

---

## âš ï¸ LÆ°u Ã½ quan trá»ng

1. **Data Requirements:**
   - Tá»‘i thiá»ƒu `look_back + 50` rows Ä‘á»ƒ train
   - Cáº§n cÃ³ Ä‘á»§ OHLC columns
   - Technical indicators sáº½ Ä‘Æ°á»£c generate tá»± Ä‘á»™ng

2. **GPU Requirements:**
   - CUDA-compatible GPU cho training nhanh
   - Mixed precision yÃªu cáº§u compute capability >= 7
   - CPU mode váº«n hoáº¡t Ä‘á»™ng nhÆ°ng cháº­m hÆ¡n

3. **Memory Management:**
   - Batch size Ä‘Æ°á»£c tá»± Ä‘á»™ng optimize
   - CNN models sá»­ dá»¥ng nhiá»u memory hÆ¡n
   - Gradient accumulation cÃ³ thá»ƒ cáº§n thiáº¿t cho GPU nhá»

4. **Model Selection:**
   - **CNN-LSTM-Attention**: Best performance, nhiá»u memory
   - **LSTM-Attention**: Balance giá»¯a performance vÃ  memory
   - **LSTM**: Fastest, Ã­t memory nháº¥t

---

## ğŸ”— Dependencies

- PyTorch
- NumPy
- Pandas
- Scikit-learn
- Custom modules:
  - `signals._components.LSTM__class__models`
  - `signals._components.LSTM__class__focal_loss`
  - `signals._components.LSTM__class__grid_search_threshold_optimizer`
  - `signals._components._generate_indicator_features`
  - `signals._components.LSTM__function__create_balanced_target`
  - `signals._components.LSTM__function__get_optimal_batch_size`
  - `signals._components._gpu_check_availability`

---

## ğŸ“ Notes

- Model Ä‘Æ°á»£c train vá»›i Focal Loss Ä‘á»ƒ handle class imbalance
- Threshold optimization sá»­ dá»¥ng Sharpe ratio lÃ m metric
- Training history Ä‘Æ°á»£c lÆ°u Ä‘á»ƒ analyze overfitting
- Scaler vÃ  feature names Ä‘Æ°á»£c lÆ°u Ä‘á»ƒ inference sau nÃ y

