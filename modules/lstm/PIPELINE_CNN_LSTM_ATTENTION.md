# CNN-LSTM-Attention Model Pipeline

## ğŸ“‹ Tá»•ng quan

Pipeline nÃ y mÃ´ táº£ quy trÃ¬nh hoÃ n chá»‰nh Ä‘á»ƒ train vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh **CNN-LSTM vá»›i Attention Mechanism** cho viá»‡c dá»± Ä‘oÃ¡n tÃ­n hiá»‡u trading (LONG/SHORT/NONE) hoáº·c dá»± Ä‘oÃ¡n return.

**Main Entry Point:** `modules/lstm/models/unified_trainer.py` - `LSTMTrainer` class

---

## ğŸ“ Cáº¥u trÃºc Modules

```text
modules/lstm/
â”œâ”€â”€ core/                    # Core components vÃ  utilities
â”‚   â”œâ”€â”€ cnn_1d_extractor.py          # CNN feature extraction
â”‚   â”œâ”€â”€ create_balanced_target.py    # Target creation cho classification
â”‚   â”œâ”€â”€ evaluate_models.py           # Model evaluation utilities
â”‚   â”œâ”€â”€ feed_forward.py              # Feed-forward layers
â”‚   â”œâ”€â”€ focal_loss.py                # Focal Loss implementation
â”‚   â”œâ”€â”€ multi_head_attention.py      # Multi-head attention mechanism
â”‚   â”œâ”€â”€ positional_encoding.py       # Positional encoding
â”‚   â””â”€â”€ threshold_optimizer.py       # Threshold optimization
â”‚
â”œâ”€â”€ models/                  # Model architectures vÃ  trainers
â”‚   â”œâ”€â”€ unified_trainer.py           # LSTMTrainer - Unified trainer for all variants
â”‚   â”œâ”€â”€ trainer/                     # Trainer components
â”‚   â”‚   â”œâ”€â”€ base_trainer.py          # BaseLSTMTrainer - Common training logic
â”‚   â”‚   â”œâ”€â”€ cnn_mixin.py             # CNNFeatureMixin - CNN-specific logic
â”‚   â”‚   â””â”€â”€ attention_mixin.py       # AttentionFeatureMixin - Attention-specific logic
â”‚   â”œâ”€â”€ model_utils.py               # Utility functions (loading, inference)
â”‚   â”œâ”€â”€ lstm_models.py               # Model definitions
â”‚   â””â”€â”€ model_factory.py             # Model factory functions
â”‚
â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”œâ”€â”€ batch_size.py                # Batch size optimization
â”‚   â”œâ”€â”€ data_utils.py                # Data splitting utilities
â”‚   â”œâ”€â”€ indicator_features.py        # Technical indicator generation
â”‚   â””â”€â”€ preprocessing.py             # Data preprocessing
â”‚
â””â”€â”€ cli/                     # Command-line interface
    â””â”€â”€ main.py                     # CLI entry point
```

---

## ğŸ”„ Workflow

```text
INPUT: OHLC DataFrame
    â†“
STEP 1: Preprocessing
    - generate_indicator_features()
    - create_balanced_target() hoáº·c tÃ­nh future return
    - preprocess_cnn_lstm_data()
    â†“
STEP 2: Data Splitting
    - split_train_test_data()
    â†“
STEP 3: Model Creation
    - create_cnn_lstm_attention_model()
    â†“
STEP 4: Training
    - LSTMTrainer.train()
    - _setup_device()
    - _prepare_tensors()
    - _setup_training_components()
    - _train_epoch()
    - _validate_epoch()
    â†“
STEP 5: Threshold Optimization
    - GridSearchThresholdOptimizer.optimize_classification_threshold()
    - GridSearchThresholdOptimizer.optimize_regression_threshold()
    â†“
STEP 6: Evaluation & Saving
    - _evaluate_model()
    - _save_model()
    â†“
OUTPUT: Trained Model + Metadata
```

---

## ğŸ”€ Unified Trainer Architecture

Táº¥t cáº£ 4 model variants Ä‘á»u sá»­ dá»¥ng cÃ¹ng má»™t `LSTMTrainer` class vá»›i cÃ¡c flags khÃ¡c nhau:

### Examples cho cÃ¡c variants

**LSTM (khÃ´ng CNN, khÃ´ng Attention):**
```python
trainer = LSTMTrainer(use_cnn=False, use_attention=False)
```

**LSTM-Attention (khÃ´ng CNN, cÃ³ Attention):**
```python
trainer = LSTMTrainer(use_cnn=False, use_attention=True, attention_heads=8)
```

**CNN-LSTM (cÃ³ CNN, khÃ´ng Attention):**
```python
trainer = LSTMTrainer(use_cnn=True, use_attention=False, look_back=60)
```

**CNN-LSTM-Attention (cÃ³ CNN, cÃ³ Attention):**
```python
trainer = LSTMTrainer(use_cnn=True, use_attention=True, attention_heads=8, look_back=60)
```

Táº¥t cáº£ Ä‘á»u dÃ¹ng cÃ¹ng interface `train()` method.

---

## ğŸ“ Chi tiáº¿t cÃ¡c bÆ°á»›c

### **STEP 1: Preprocessing**

**Functions:**

- `modules.lstm.utils.indicator_features.generate_indicator_features()` - Táº¡o technical indicators
- `modules.lstm.core.create_balanced_target.create_balanced_target()` - Táº¡o classification targets (cho classification mode)
- `modules.lstm.utils.preprocessing.preprocess_cnn_lstm_data()` - Chuáº©n bá»‹ sequences vÃ  scaling

**Output:**

- `X_sequences`: Sequences array `(n_samples, look_back, num_features)`
- `y_targets`: Target array `(n_samples,)`
- `scaler`: Fitted scaler
- `feature_names`: List feature names

---

### **STEP 2: Data Splitting**

**Function:**

- `modules.lstm.utils.data_utils.split_train_test_data()` - Chia train/validation/test

**Output:**

- `X_train, X_val, X_test, y_train, y_val, y_test`

---

### **STEP 3: Model Creation**

**Function:**

- `modules.lstm.models.model_factory.create_cnn_lstm_attention_model()` - Táº¡o model architecture

**Models (tá»« `modules.lstm.models.lstm_models`):**

- `CNNLSTMAttentionModel` (khi `use_cnn=True`)
- `LSTMAttentionModel` (khi `use_attention=True` vÃ  `use_cnn=False`)
- `LSTMModel` (standard LSTM)

---

### **STEP 4: Training**

**Class:** `LSTMTrainer` (tá»« `modules/lstm/models/unified_trainer.py`)

Unified trainer há»— trá»£ táº¥t cáº£ 4 variants:
- LSTM (use_cnn=False, use_attention=False)
- LSTM-Attention (use_cnn=False, use_attention=True)
- CNN-LSTM (use_cnn=True, use_attention=False)
- CNN-LSTM-Attention (use_cnn=True, use_attention=True)

**Methods:**

- `_setup_device()` - Setup GPU vÃ  mixed precision (tá»« BaseLSTMTrainer)
- `_prepare_tensors()` - Chuyá»ƒn data thÃ nh tensors (tá»« BaseLSTMTrainer)
- `create_model()` - Táº¡o model dá»±a trÃªn flags (use_cnn, use_attention)
- `_setup_training_components()` - Setup optimizer, scheduler, loss function
- `_train_epoch()` - Train má»™t epoch (tá»« BaseLSTMTrainer)
- `_validate_epoch()` - Validate má»™t epoch (tá»« BaseLSTMTrainer)
- `train()` - Main training loop vá»›i early stopping

**Helper Functions:**

- `modules.common.utils.system.detect_pytorch_gpu_availability()` - Detect GPU
- `modules.lstm.utils.batch_size.get_optimal_batch_size()` - Tá»‘i Æ°u batch size
- `CNNFeatureMixin._adjust_batch_size_for_cnn()` - Äiá»u chá»‰nh batch size cho CNN models

---

### **STEP 5: Threshold Optimization**

**Class:** `modules.lstm.core.threshold_optimizer.GridSearchThresholdOptimizer`

**Methods:**

- `optimize_classification_threshold()` - Tá»‘i Æ°u cho classification mode
- `optimize_regression_threshold()` - Tá»‘i Æ°u cho regression mode

---

### **STEP 6: Evaluation & Saving**

**Methods:**

- `_evaluate_model()` - ÄÃ¡nh giÃ¡ model trÃªn test set
- `_save_model()` - LÆ°u model vÃ  metadata

---

## ğŸ¯ Main Functions

### 1. `preprocess_cnn_lstm_data()`

**Location:** `modules/lstm/utils/preprocessing.py`

Chuáº©n bá»‹ dá»¯ liá»‡u cho training.

### 2. `split_train_test_data()`

**Location:** `modules/lstm/utils/data_utils.py`

Chia dá»¯ liá»‡u thÃ nh train/validation/test.

### 3. `create_cnn_lstm_attention_model()`

**Location:** `modules/lstm/models/model_factory.py`

Táº¡o model architecture.

### 4. `LSTMTrainer.train()`

**Location:** `modules/lstm/models/unified_trainer.py`

Train model hoÃ n chá»‰nh cho táº¥t cáº£ variants.

### 5. `GridSearchThresholdOptimizer`

**Location:** `modules/lstm/core/threshold_optimizer.py`

Tá»‘i Æ°u threshold cho trading signals.

---

## âš™ï¸ Configuration

CÃ¡c constants tá»« `config.lstm`:

- `WINDOW_SIZE_LSTM` - Default look_back
- `TARGET_THRESHOLD_LSTM` - Threshold cho classification targets
- `NEUTRAL_ZONE_LSTM` - Neutral zone cho balanced targets
- `TRAIN_TEST_SPLIT` - Train ratio (0.7)
- `VALIDATION_SPLIT` - Validation ratio (0.15)
- `DEFAULT_EPOCHS` - Default sá»‘ epochs

---

## ğŸš€ Example Usage

```python
from modules.lstm.models import LSTMTrainer
import pandas as pd

# Load data
df = pd.read_csv('price_data.csv')

# Create unified trainer for CNN-LSTM-Attention
trainer = LSTMTrainer(
    use_cnn=True,
    use_attention=True,
    output_mode='classification',
    look_back=60
)

# Train
model, threshold_optimizer, model_path = trainer.train(
    df_input=df,
    epochs=100,
    save_model=True
)
```

---

## ğŸ“Š Model Architecture

```text
Input: (batch_size, look_back, num_features)
    â†“
CNN1DExtractor (core.cnn_1d_extractor) - náº¿u use_cnn=True
    â†“
LSTM Layers (models.lstm_models)
    â†“
PositionalEncoding (core.positional_encoding) - náº¿u use_attention=True
    â†“
MultiHeadAttention (core.multi_head_attention) - náº¿u use_attention=True
    â†“
FeedForward (core.feed_forward)
    â†“
Output: 
  - Classification: (batch_size, 3) - [SELL, NEUTRAL, BUY]
  - Regression: (batch_size, 1) - [Return]
```

**Loss Functions:**

- Classification: `FocalLoss` (core.focal_loss)
- Regression: `nn.MSELoss` (PyTorch)

---

## ğŸ”— Module Dependencies

### Core Modules (`modules/lstm/core/`)

- `core.cnn_1d_extractor` - CNN1DExtractor class cho feature extraction
- `core.create_balanced_target` - `create_balanced_target()` function
- `core.evaluate_models` - `evaluate_model_with_confidence()`, `apply_confidence_threshold()`
- `core.feed_forward` - FeedForward class
- `core.focal_loss` - FocalLoss class
- `core.multi_head_attention` - MultiHeadAttention class
- `core.positional_encoding` - PositionalEncoding class
- `core.threshold_optimizer` - GridSearchThresholdOptimizer class

### Model Modules (`modules/lstm/models/`)

- `models.unified_trainer` - `LSTMTrainer` class (unified trainer cho táº¥t cáº£ variants)
- `models.trainer.base_trainer` - `BaseLSTMTrainer` class (common training logic)
- `models.trainer.cnn_mixin` - `CNNFeatureMixin` class (CNN-specific logic)
- `models.trainer.attention_mixin` - `AttentionFeatureMixin` class (attention-specific logic)
- `models.model_utils` - Utility functions:
  - `load_lstm_model()` - Load trained model tá»« checkpoint
  - `get_latest_signal()` - Generate trading signals tá»« model
- `models.lstm_models` - `LSTMModel`, `LSTMAttentionModel`, `CNNLSTMAttentionModel` classes
- `models.model_factory` - `create_cnn_lstm_attention_model()` function

### Utility Modules (`modules/lstm/utils/`)

- `utils.preprocessing` - `preprocess_cnn_lstm_data()` function
- `utils.data_utils` - `split_train_test_data()` function
- `utils.indicator_features` - `generate_indicator_features()` function
- `utils.batch_size` - `get_optimal_batch_size()` function

### CLI (`modules/lstm/cli/`)

- `cli.main` - Command-line interface entry point

### External Dependencies

- PyTorch
- NumPy, Pandas
- Scikit-learn
- Custom modules:
  - `modules.common.utils.system` - GPU detection utilities
  - `modules.common.ui.logging` - Logging utilities
  - `config.lstm` - Configuration constants
  - `config.model_features` - Feature definitions
