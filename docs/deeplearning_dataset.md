# üìö Deep Learning Dataset Documentation

## M·ª•c l·ª•c
1. [T·ªïng quan](#t·ªïng-quan)
2. [Kh·ªüi t·∫°o](#kh·ªüi-t·∫°o)
3. [Ph∆∞∆°ng th·ª©c ch√≠nh](#ph∆∞∆°ng-th·ª©c-ch√≠nh)
4. [TimeSeriesDataSet Creation](#timeseriesdataset-creation)
5. [V√≠ d·ª• s·ª≠ d·ª•ng](#v√≠-d·ª•-s·ª≠-d·ª•ng)
6. [Best Practices](#best-practices)
7. [X·ª≠ l√Ω Missing Candles](#x·ª≠-l√Ω-missing-candles)
8. [Time Index Calculation](#time-index-calculation)

---

## T·ªïng quan

`TFTDataModule` l√† m·ªôt PyTorch Lightning DataModule ƒë·ªÉ t·∫°o TimeSeriesDataSet cho Temporal Fusion Transformer (TFT). Module n√†y cung c·∫•p:

- ‚úÖ **TimeSeriesDataSet Creation** - T·ª± ƒë·ªông t·∫°o dataset v·ªõi ƒë√∫ng format cho TFT
- ‚úÖ **Time Index Handling** - X·ª≠ l√Ω `time_idx` v·ªõi missing candle resampling
- ‚úÖ **Multi-asset Support** - H·ªó tr·ª£ training nhi·ªÅu symbols c√πng l√∫c
- ‚úÖ **Missing Candle Handling** - Resample v√† fill gaps th√¥ng minh
- ‚úÖ **Feature Categorization** - T·ª± ƒë·ªông ph√¢n lo·∫°i known/unknown future features
- ‚úÖ **DataLoaders** - Train/validation/test DataLoaders v·ªõi batch size configurable
- ‚úÖ **Metadata Persistence** - L∆∞u dataset metadata ƒë·ªÉ d√πng cho inference

### Khi n√†o d√πng TFTDataModule?

| M·ª•c ƒë√≠ch | D√πng TFTDataModule? | Ph∆∞∆°ng th·ª©c |
|----------|---------------------|-------------|
| T·∫°o TimeSeriesDataSet cho TFT | ‚úÖ C√≥ | `setup()` + `_create_dataset()` |
| C·∫ßn train/val/test DataLoaders | ‚úÖ C√≥ | `train_dataloader()`, `val_dataloader()`, `test_dataloader()` |
| C·∫ßn x·ª≠ l√Ω missing candles | ‚úÖ C√≥ | T·ª± ƒë·ªông trong `prepare_data()` |
| C·∫ßn multi-asset training | ‚úÖ C√≥ | `group_ids=["symbol"]` |
| C·∫ßn l∆∞u dataset metadata | ‚úÖ C√≥ | `save_dataset_metadata()` |

---

## Kh·ªüi t·∫°o

### C√∫ ph√°p

```python
from modules.deeplearning_dataset import TFTDataModule, create_tft_datamodule

# C√°ch 1: S·ª≠ d·ª•ng convenience function (khuy·∫øn ngh·ªã)
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    target_col="future_log_return",
    task_type="regression",
    timeframe="1h"
)

# C√°ch 2: Kh·ªüi t·∫°o tr·ª±c ti·∫øp
datamodule = TFTDataModule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    target_col="future_log_return",
    task_type="regression",
    max_encoder_length=64,
    max_prediction_length=24,
    batch_size=64,
    timeframe="1h"
)
```

### Tham s·ªë ch√≠nh

- `train_df` (pd.DataFrame, **b·∫Øt bu·ªôc**): Training DataFrame (ƒë√£ preprocess)
- `val_df` (pd.DataFrame, **b·∫Øt bu·ªôc**): Validation DataFrame (ƒë√£ preprocess)
- `test_df` (pd.DataFrame, **t√πy ch·ªçn**): Test DataFrame (ƒë√£ preprocess)
- `target_col` (str, **m·∫∑c ƒë·ªãnh**: `"future_log_return"`): Target column name
- `task_type` (str, **m·∫∑c ƒë·ªãnh**: `"regression"`): `"regression"` ho·∫∑c `"classification"`
- `max_encoder_length` (int, **m·∫∑c ƒë·ªãnh**: `64`): Lookback window length (64-128 recommended)
- `max_prediction_length` (int, **m·∫∑c ƒë·ªãnh**: `24`): Prediction horizon (align v·ªõi TARGET_HORIZON)
- `batch_size` (int, **m·∫∑c ƒë·ªãnh**: `64`): Batch size cho DataLoaders
- `num_workers` (int, **m·∫∑c ƒë·ªãnh**: `4`): S·ªë workers cho DataLoader
- `timeframe` (str, **t√πy ch·ªçn**): Timeframe string (v√≠ d·ª•: `"1h"`, `"4h"`) ƒë·ªÉ t√≠nh time_idx ch√≠nh x√°c
- `allow_missing_timesteps` (bool, **m·∫∑c ƒë·ªãnh**: `False`): Cho ph√©p missing timesteps (n·∫øu False, s·∫Ω resample)
- `max_ffill_limit` (int, **m·∫∑c ƒë·ªãnh**: `5`): Gi·ªõi h·∫°n forward fill ƒë·ªÉ tr√°nh infinite fill
- `use_interpolation` (bool, **m·∫∑c ƒë·ªãnh**: `True`): D√πng linear interpolation cho gaps ng·∫Øn
- `max_gap_candles` (int, **m·∫∑c ƒë·ªãnh**: `10`): K√≠ch th∆∞·ªõc gap t·ªëi ƒëa ƒë·ªÉ d√πng interpolation

### V√≠ d·ª• kh·ªüi t·∫°o

```python
from modules.deeplearning_dataset import create_tft_datamodule

# C√°ch 1: M·∫∑c ƒë·ªãnh
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df
)

# C√°ch 2: T√πy ch·ªânh
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    target_col="triple_barrier_label",
    task_type="classification",
    max_encoder_length=128,
    max_prediction_length=24,
    batch_size=32,
    timeframe="1h",
    use_interpolation=True,
    max_ffill_limit=3
)

# C√°ch 3: Cho prediction (kh√¥ng c·∫ßn target)
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=None  # Kh√¥ng c√≥ test set
)
```

---

## Ph∆∞∆°ng th·ª©c ch√≠nh

### `prepare_data() -> None`

Chu·∫©n b·ªã data: resample missing candles v√† t·∫°o time_idx.

**L∆∞u √Ω:** ƒê∆∞·ª£c g·ªçi t·ª± ƒë·ªông b·ªüi PyTorch Lightning, nh∆∞ng c√≥ th·ªÉ g·ªçi th·ªß c√¥ng.

**V√≠ d·ª•:**

```python
datamodule.prepare_data()  # Resample v√† t·∫°o time_idx
```

### `setup(stage=None) -> None`

Setup datasets cho training, validation, v√† testing.

**Tham s·ªë:**
- `stage` (str, **t√πy ch·ªçn**): `"fit"` ho·∫∑c `"test"`

**V√≠ d·ª•:**

```python
# Setup cho training
datamodule.setup("fit")

# Setup cho testing
datamodule.setup("test")
```

### `train_dataloader() -> DataLoader`

Tr·∫£ v·ªÅ training DataLoader.

**L∆∞u √Ω:** Ph·∫£i g·ªçi `setup("fit")` tr∆∞·ªõc.

**V√≠ d·ª•:**

```python
datamodule.setup("fit")
train_loader = datamodule.train_dataloader()

for batch in train_loader:
    # Training loop
    pass
```

### `val_dataloader() -> DataLoader`

Tr·∫£ v·ªÅ validation DataLoader.

**V√≠ d·ª•:**

```python
val_loader = datamodule.val_dataloader()
```

### `test_dataloader() -> DataLoader`

Tr·∫£ v·ªÅ test DataLoader.

**L∆∞u √Ω:** Ph·∫£i c√≥ `test_df` v√† g·ªçi `setup("test")` tr∆∞·ªõc.

**V√≠ d·ª•:**

```python
datamodule.setup("test")
test_loader = datamodule.test_dataloader()
```

### `save_dataset_metadata(filepath=None) -> None`

L∆∞u dataset metadata ƒë·ªÉ d√πng cho inference sau n√†y.

**Tham s·ªë:**
- `filepath` (str, **t√πy ch·ªçn**): ƒê∆∞·ªùng d·∫´n file (m·∫∑c ƒë·ªãnh: `artifacts/deep/datasets/dataset_metadata.pkl`)

**V√≠ d·ª•:**

```python
datamodule.setup("fit")
datamodule.save_dataset_metadata()
```

### `load_dataset_metadata(filepath=None) -> Dict`

Load dataset metadata ƒë√£ l∆∞u.

**V√≠ d·ª•:**

```python
metadata = datamodule.load_dataset_metadata()
print(metadata["target_col"])
print(metadata["max_encoder_length"])
```

### `get_dataset_info() -> Dict`

L·∫•y th√¥ng tin v·ªÅ datasets.

**V√≠ d·ª•:**

```python
info = datamodule.get_dataset_info()
print(info)
# {
#     "train_samples": 1000,
#     "val_samples": 200,
#     "test_samples": 200,
#     "max_encoder_length": 64,
#     "max_prediction_length": 24,
#     ...
# }
```

---

## TimeSeriesDataSet Creation

### Feature Categorization

Module t·ª± ƒë·ªông ph√¢n lo·∫°i features th√†nh:

1. **Time-varying Known Reals**: Features bi·∫øt tr∆∞·ªõc (known future)
   - `hour_sin`, `hour_cos`
   - `day_sin`, `day_cos`
   - `day_of_month_sin`, `day_of_month_cos`
   - `hours_to_funding`, `is_funding_time`
   - `candle_index`

2. **Time-varying Unknown Reals**: Features kh√¥ng bi·∫øt tr∆∞·ªõc
   - Price features: `open`, `high`, `low`, `close`, `volume`
   - Technical indicators: `SMA_20`, `RSI_14`, `MACD_12_26_9`, etc.
   - Volatility metrics: `volatility_20`, `volatility_50`

3. **Static Reals**: Features kh√¥ng ƒë·ªïi theo time series
   - Hi·ªán t·∫°i empty (c√≥ th·ªÉ th√™m sau n·∫øu c·∫ßn)

4. **Categorical Features**: Categorical variables
   - Hi·ªán t·∫°i empty (c√≥ th·ªÉ th√™m sau n·∫øu c·∫ßn)

### Target Normalization

- **Regression**: S·ª≠ d·ª•ng `GroupNormalizer` v·ªõi `transformation="softplus"` per symbol
- **Classification**: Kh√¥ng normalize (None)

---

## V√≠ d·ª• s·ª≠ d·ª•ng

### V√≠ d·ª• 1: Basic Workflow

```python
from modules.deeplearning_dataset import create_tft_datamodule
from modules.deeplearning_data_pipeline import DeepLearningDataPipeline

# 1. Prepare data
pipeline = DeepLearningDataPipeline(data_fetcher)
df = pipeline.fetch_and_prepare(symbols=["BTC/USDT"], timeframe="1h")
train_df, val_df, test_df = pipeline.split_chronological(df)

# 2. Create DataModule
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    timeframe="1h"
)

# 3. Setup v√† s·ª≠ d·ª•ng
datamodule.prepare_data()
datamodule.setup("fit")

# 4. Get DataLoaders
train_loader = datamodule.train_dataloader()
val_loader = datamodule.val_dataloader()

# 5. Training loop
for batch in train_loader:
    # Your training code
    pass
```

### V√≠ d·ª• 2: V·ªõi PyTorch Lightning Trainer

```python
import pytorch_lightning as pl
from pytorch_forecasting import TemporalFusionTransformer

# Create DataModule
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    timeframe="1h"
)

# Create model
model = TemporalFusionTransformer.from_dataset(
    datamodule.training,
    learning_rate=0.03,
    hidden_size=16,
    attention_head_size=4,
    dropout=0.1
)

# Train
trainer = pl.Trainer(max_epochs=10)
trainer.fit(model, datamodule=datamodule)
```

### V√≠ d·ª• 3: Multi-asset Training

```python
# Fetch nhi·ªÅu symbols
df = pipeline.fetch_and_prepare(
    symbols=["BTC/USDT", "ETH/USDT", "BNB/USDT"],
    timeframe="4h"
)
train_df, val_df, test_df = pipeline.split_chronological(df)

# Create DataModule (group_ids=["symbol"] t·ª± ƒë·ªông)
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    timeframe="4h"
)

datamodule.setup("fit")
# Model s·∫Ω train tr√™n t·∫•t c·∫£ symbols c√πng l√∫c
```

### V√≠ d·ª• 4: Classification Task

```python
# V·ªõi triple barrier labels
pipeline = DeepLearningDataPipeline(
    data_fetcher=data_fetcher,
    use_triple_barrier=True
)

df = pipeline.fetch_and_prepare(symbols=["BTC/USDT"])
train_df, val_df, test_df = pipeline.split_chronological(
    df,
    target_col="triple_barrier_label",
    task_type="classification"
)

datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    target_col="triple_barrier_label",
    task_type="classification",
    timeframe="1h"
)
```

### V√≠ d·ª• 5: T√πy ch·ªânh Gap Handling

```python
# T·∫Øt interpolation, ch·ªâ d√πng limited ffill
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    use_interpolation=False,
    max_ffill_limit=3,  # Ch·ªâ fill t·ªëi ƒëa 3 candles
    timeframe="1h"
)

# Ho·∫∑c cho ph√©p missing timesteps
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    test_df=test_df,
    allow_missing_timesteps=True,  # Kh√¥ng resample
    timeframe="1h"
)
```

---

## Best Practices

### 1. Timeframe Specification

**Lu√¥n ch·ªâ ƒë·ªãnh `timeframe`** ƒë·ªÉ ƒë·∫£m b·∫£o `time_idx` ch√≠nh x√°c:

```python
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    timeframe="1h"  # Quan tr·ªçng!
)
```

### 2. Missing Candle Handling

- **M·∫∑c ƒë·ªãnh (`allow_missing_timesteps=False`)**: Resample v√† fill gaps
- **V·ªõi gaps l·ªõn**: S·ª≠ d·ª•ng `max_ffill_limit` ƒë·ªÉ tr√°nh infinite fill
- **V·ªõi gaps nh·ªè**: B·∫≠t `use_interpolation=True` ƒë·ªÉ smooth h∆°n

### 3. Multi-asset Training

- ƒê·∫£m b·∫£o data ƒë√£ ƒë∆∞·ª£c normalize per symbol (t·ª´ pipeline)
- `group_ids=["symbol"]` t·ª± ƒë·ªông ƒë∆∞·ª£c set
- M·ªói symbol c√≥ `time_idx` b·∫Øt ƒë·∫ßu t·ª´ 0

### 4. Encoder/Prediction Length

- **max_encoder_length**: 64-128 bars (khuy·∫øn ngh·ªã)
- **max_prediction_length**: Ph·∫£i align v·ªõi `TARGET_HORIZON`
- ƒê·∫£m b·∫£o c√≥ ƒë·ªß data: `len(df) >= max_encoder_length + max_prediction_length`

### 5. Batch Size

- **GPU**: 32-128 (t√πy GPU memory)
- **CPU**: 16-64
- L·ªõn h∆°n = nhanh h∆°n nh∆∞ng c·∫ßn nhi·ªÅu memory h∆°n

---

## X·ª≠ l√Ω Missing Candles

### Strategy

Module s·ª≠ d·ª•ng chi·∫øn l∆∞·ª£c th√¥ng minh ƒë·ªÉ x·ª≠ l√Ω missing candles:

1. **Short gaps (‚â§ max_gap_candles)**: Linear interpolation (n·∫øu `use_interpolation=True`)
2. **Medium gaps**: Limited forward fill (`max_ffill_limit`)
3. **Large gaps**: Gi·ªØ NaN (t·ªët h∆°n l√† t·∫°o artificial flat data)

### V√≠ d·ª•

```python
# V·ªõi interpolation cho gaps ng·∫Øn
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    use_interpolation=True,
    max_gap_candles=10,  # Interpolate cho gaps ‚â§ 10 candles
    max_ffill_limit=5    # Ffill t·ªëi ƒëa 5 candles
)

# Ch·ªâ d√πng ffill (kh√¥ng interpolation)
datamodule = create_tft_datamodule(
    train_df=train_df,
    val_df=val_df,
    use_interpolation=False,
    max_ffill_limit=3
)
```

### T·∫°i sao c·∫ßn limit ffill?

- **V√¥ h·∫°n ffill**: T·∫°o artificial flat data khi c√≥ gaps l·ªõn (v√≠ d·ª•: exchange maintenance)
- **Limited ffill**: Ch·ªâ fill gaps nh·ªè, gi·ªØ NaN cho gaps l·ªõn (an to√†n h∆°n)

---

## Time Index Calculation

### S·ª≠ d·ª•ng candle_index t·ª´ Pipeline

Module ∆∞u ti√™n s·ª≠ d·ª•ng `candle_index` t·ª´ `DeepLearningDataPipeline`:

1. **N·∫øu c√≥ `candle_index`**: S·ª≠ d·ª•ng v√† normalize per symbol
2. **N·∫øu kh√¥ng c√≥**: T√≠nh t·ª´ timestamps (fallback)

### Normalization Per Symbol

M·ªói symbol c√≥ `time_idx` b·∫Øt ƒë·∫ßu t·ª´ 0:

```python
# Symbol 1: candle_index = [100, 101, 102, ...]
# ‚Üí time_idx = [0, 1, 2, ...]

# Symbol 2: candle_index = [200, 201, 202, ...]
# ‚Üí time_idx = [0, 1, 2, ...]  # C≈©ng b·∫Øt ƒë·∫ßu t·ª´ 0
```

### T√≠nh nh·∫•t qu√°n

- `time_idx`: D√πng cho TimeSeriesDataSet ordering
- `candle_index`: D√πng l√†m known future feature
- C·∫£ hai ƒë·ªÅu d·ª±a tr√™n c√πng logic t√≠nh to√°n (t·ª´ timestamp)

---

## Configuration

C√°c config constants trong `modules/config.py`:

```python
# Dataset Configuration
DEEP_MAX_ENCODER_LENGTH = 64  # Lookback window
DEEP_MAX_PREDICTION_LENGTH = 24  # Prediction horizon (TARGET_HORIZON)
DEEP_BATCH_SIZE = 64  # Batch size
DEEP_NUM_WORKERS = 4  # DataLoader workers
DEEP_TARGET_COL = "future_log_return"  # Default target
DEEP_TARGET_COL_CLASSIFICATION = "triple_barrier_label"  # Classification target
DEEP_DATASET_DIR = "artifacts/deep/datasets"  # Metadata directory
```

---

## Troubleshooting

### L·ªói: "Must call setup('fit') first"

**Nguy√™n nh√¢n:** Ch∆∞a g·ªçi `setup()` tr∆∞·ªõc khi d√πng DataLoader

**Gi·∫£i ph√°p:**
```python
datamodule.setup("fit")  # Ph·∫£i g·ªçi tr∆∞·ªõc
train_loader = datamodule.train_dataloader()
```

### L·ªói: "Target column not found"

**Nguy√™n nh√¢n:** Target column kh√¥ng t·ªìn t·∫°i trong DataFrame

**Gi·∫£i ph√°p:**
- Ki·ªÉm tra `target_col` parameter
- ƒê·∫£m b·∫£o ƒë√£ preprocess data ƒë√∫ng c√°ch
- V·ªõi classification, d√πng `"triple_barrier_label"`

### L·ªói: "No valid features after filtering"

**Nguy√™n nh√¢n:** Kh√¥ng c√≥ features h·ª£p l·ªá sau khi filter

**Gi·∫£i ph√°p:**
- Ki·ªÉm tra data quality
- ƒê·∫£m b·∫£o c√≥ numeric features
- Ki·ªÉm tra c√≥ target leakage columns kh√¥ng

### Time_idx issues

**Nguy√™n nh√¢n:** `time_idx` kh√¥ng li√™n t·ª•c ho·∫∑c c√≥ gaps

**Gi·∫£i ph√°p:**
- ƒê·∫£m b·∫£o `allow_missing_timesteps=False` (m·∫∑c ƒë·ªãnh)
- Ch·ªâ ƒë·ªãnh `timeframe` ƒë·ªÉ t√≠nh ch√≠nh x√°c
- Ki·ªÉm tra data c√≥ missing candles kh√¥ng

---

## Tham kh·∫£o

- [PyTorch Forecasting Documentation](https://pytorch-forecasting.readthedocs.io/)
- [Temporal Fusion Transformer Paper](https://arxiv.org/abs/1912.09363)
- [PyTorch Lightning DataModule](https://pytorch-lightning.readthedocs.io/en/stable/data/datamodule.html)

